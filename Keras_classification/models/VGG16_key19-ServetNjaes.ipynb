{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img, img_to_array\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 448 training samples, 150 validation samples and 150 test samples\n"
     ]
    }
   ],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "weights_path = '../saved_weights/vgg16_weights.h5'\n",
    "top_model_weights_path = '../saved_weights/bottleneck_fc_model_servet.h5'\n",
    "train_data_dir = '../data/model/key19/train'\n",
    "validation_data_dir = '../data/model/key19/test'\n",
    "test_data_dir = '../data/model/key19/test'\n",
    "nb_train_samples = int(len(glob.glob(train_data_dir+'/*/*')))\n",
    "nb_validation_samples = int(len(glob.glob(validation_data_dir+'/*/*')))\n",
    "nb_test_samples = int(len(glob.glob(test_data_dir+'/*/*')))\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "print('Found {} training samples, {} validation samples and {} test samples'.format(nb_train_samples,nb_validation_samples, nb_test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of files in each folder\n",
    "nb_train_sub_samples = np.zeros(6)\n",
    "nb_validation_sub_samples = np.zeros(6)\n",
    "for i in range(1,7):\n",
    "    path_train = train_data_dir+'/pos'+str(i)+'/*.png'\n",
    "    path_validation = validation_data_dir+'/pos'+str(i)+'/*.png'\n",
    "    nb_train_sub_samples[i-1] = int(len(glob.glob(path_train)))\n",
    "    nb_validation_sub_samples[i-1] = int(len(glob.glob(path_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False,input_shape = (img_width, img_height, 3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(4096, activation='relu'))\n",
    "top_model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../saved_weights/bottleneck_fc_model_servet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ede3614dc577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# classifier, including the top classifier,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# in order to successfully do fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../saved_weights/bottleneck_fc_model_servet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model = Model(inputs= base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4, rho=0.9,epsilon=1e-08, decay=0.0))\n",
    "\n",
    "from keras import metrics\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), metrics = [metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(#preprocessing_function=subtractmean,\n",
    "                                    rotation_range=10,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    fill_mode='nearest',\n",
    "                                    zca_whitening=True,\n",
    "                                    featurewise_std_normalization = True,\n",
    "                                    #horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(#preprocessing_function=subtractmean,\n",
    "                                        #rotation_range=0,\n",
    "                                        #width_shift_range=0,\n",
    "                                        #height_shift_range=0,\n",
    "                                        #shear_range=0,\n",
    "                                        #zoom_range=0,\n",
    "                                        #zca_whitening=False,\n",
    "                                        #featurewise_std_normalization = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 293 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='../saved_weights/VGG16_key19_weights_servet.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import ndimage, misc\n",
    "image_paths = glob.glob('../data/model/key19/train/*/*')\n",
    "x = []\n",
    "#example(2000, 256, 256, 3)\n",
    "#train_datagen.fit()\n",
    "for image in image_paths:\n",
    "    img = load_img(image, target_size=(256,256))\n",
    "    buf = img_to_array(img)\n",
    "    x.append(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen.fit(x)\n",
    "validation_datagen.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "train_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('transfer_key19_99_6p_2.h5')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../saved_weights/VGG16_key19_weights_servet.hdf5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate_generator(validation_generator, \n",
    "                                steps = nb_validation_samples // batch_size)\n",
    "print()\n",
    "print('Validation Loss:', loss)\n",
    "print('Validation Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=1,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')\n",
    "val_pred_prob = model.predict_generator(validation_generator,steps = nb_validation_samples, verbose=1)\n",
    "val_pred_labels = np.argmax(val_pred_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 1-np.sum(val_pred_labels!=validation_generator.classes[:nb_validation_samples])/nb_validation_samples\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array(np.where(val_pred_labels!=validation_generator.classes))\n",
    "for i in range(0,idx.shape[1]):\n",
    "    print(validation_generator.filenames[idx[0,i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=subtractmean)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=1,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_prob = model.predict_generator(test_generator,steps = nb_test_samples, verbose=1)\n",
    "test_pred_labels = np.argmax(test_pred_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 1-np.sum(test_pred_labels!=test_generator.classes[:nb_test_samples])/nb_test_samples\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array(np.where(test_pred_labels!=test_generator.classes))\n",
    "for i in range(0,idx.shape[1]):\n",
    "    print(test_generator.filenames[idx[0,i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False,input_shape = (img_width, img_height, 3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model = Model(inputs= base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../saved_weights/VGG16_key19_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=1,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_prob = model.predict_generator(test_generator,steps = nb_test_samples, verbose=1)\n",
    "test_pred_labels = np.argmax(test_pred_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 1-np.sum(test_pred_labels!=test_generator.classes[:nb_test_samples])/nb_test_samples\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array(np.where(test_pred_labels!=test_generator.classes))\n",
    "for i in range(0,idx.shape[1]):\n",
    "    print('Classified as pos {}'.format(test_pred_labels[i]+1))\n",
    "    filename = '../data/model/key19/test/'+test_generator.filenames[idx[0,i]]\n",
    "    plt.figure()\n",
    "    image = imread(filename)\n",
    "    plt.imshow(image)\n",
    "    plt.title(filename)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for creating the model init weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtractmean(img):\n",
    "   \n",
    "    mean_image = np.mean(img)\n",
    "    img -= mean_image\n",
    "    img /= 128.\n",
    "    \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 448 images belonging to 6 classes.\n",
      "Found 150 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(#preprocessing_function=subtractmean\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(#preprocessing_function=subtractmean\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle = False)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(#preprocessing_function=subtractmean\n",
    "    )\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size, verbose=1)\n",
    "    \n",
    "    np.save(open('../saved_weights/bottleneck_features_train_servet.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples // batch_size, verbose=1)\n",
    "    np.save(open('../saved_weights/bottleneck_features_validation_servet.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load(open('../saved_weights/bottleneck_features_train_servet.npy','rb'))\n",
    "    train_labels = np.array([[1,0,0,0,0,0]] * nb_train_sub_samples[0].astype(int) + \n",
    "                            [[0,1,0,0,0,0]] * nb_train_sub_samples[1].astype(int) +\n",
    "                            [[0,0,1,0,0,0]] * nb_train_sub_samples[2].astype(int) + \n",
    "                            [[0,0,0,1,0,0]] * nb_train_sub_samples[3].astype(int) +\n",
    "                            [[0,0,0,0,1,0]] * nb_train_sub_samples[4].astype(int) + \n",
    "                            [[0,0,0,0,0,1]] * nb_train_sub_samples[5].astype(int))\n",
    "    train_labels = train_labels[0:nb_train_samples // batch_size * batch_size]\n",
    "\n",
    "    validation_data = np.load(open('../saved_weights/bottleneck_features_validation_servet.npy','rb'))\n",
    "    validation_labels = np.array([[1,0,0,0,0,0]] * nb_validation_sub_samples[0].astype(int) + \n",
    "                                 [[0,1,0,0,0,0]] * nb_validation_sub_samples[1].astype(int) +\n",
    "                                 [[0,0,1,0,0,0]] * nb_validation_sub_samples[2].astype(int) + \n",
    "                                 [[0,0,0,1,0,0]] * nb_validation_sub_samples[3].astype(int) +\n",
    "                                 [[0,0,0,0,1,0]] * nb_validation_sub_samples[4].astype(int) + \n",
    "                                 [[0,0,0,0,0,1]] * nb_validation_sub_samples[5].astype(int))\n",
    "    validation_labels = validation_labels[0:nb_validation_samples // batch_size * batch_size]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 448 images belonging to 6 classes.\n",
      "7/7 [==============================] - 123s    \n",
      "Found 150 images belonging to 6 classes.\n",
      "2/2 [==============================] - 35s     \n"
     ]
    }
   ],
   "source": [
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 448 samples, validate on 128 samples\n",
      "Epoch 1/100\n",
      "448/448 [==============================] - 8s - loss: 11.5423 - acc: 0.2701 - val_loss: 13.4738 - val_acc: 0.1641\n",
      "Epoch 2/100\n",
      "448/448 [==============================] - 6s - loss: 11.9001 - acc: 0.2612 - val_loss: 14.6070 - val_acc: 0.0938\n",
      "Epoch 3/100\n",
      "448/448 [==============================] - 6s - loss: 13.3478 - acc: 0.1719 - val_loss: 14.7329 - val_acc: 0.0859\n",
      "Epoch 4/100\n",
      "448/448 [==============================] - 6s - loss: 13.9234 - acc: 0.1362 - val_loss: 14.7329 - val_acc: 0.0859\n",
      "Epoch 5/100\n",
      "448/448 [==============================] - 6s - loss: 14.1033 - acc: 0.1250 - val_loss: 14.8589 - val_acc: 0.0781\n",
      "Epoch 6/100\n",
      "448/448 [==============================] - 6s - loss: 14.0674 - acc: 0.1272 - val_loss: 14.9848 - val_acc: 0.0703\n",
      "Epoch 7/100\n",
      "448/448 [==============================] - 6s - loss: 13.9954 - acc: 0.1317 - val_loss: 15.1107 - val_acc: 0.0625\n",
      "Epoch 8/100\n",
      "448/448 [==============================] - 6s - loss: 14.0674 - acc: 0.1272 - val_loss: 15.1107 - val_acc: 0.0625\n",
      "Epoch 9/100\n",
      "448/448 [==============================] - 6s - loss: 14.1033 - acc: 0.1250 - val_loss: 14.8589 - val_acc: 0.0781\n",
      "Epoch 10/100\n",
      "448/448 [==============================] - 6s - loss: 14.0674 - acc: 0.1272 - val_loss: 14.7329 - val_acc: 0.0859\n",
      "Epoch 11/100\n",
      "448/448 [==============================] - 6s - loss: 13.9234 - acc: 0.1362 - val_loss: 14.7329 - val_acc: 0.0859\n",
      "Epoch 12/100\n",
      "448/448 [==============================] - 6s - loss: 13.7436 - acc: 0.1473 - val_loss: 14.7329 - val_acc: 0.0859\n",
      "Epoch 13/100\n",
      "448/448 [==============================] - 6s - loss: 12.6626 - acc: 0.2143 - val_loss: 11.2071 - val_acc: 0.3047\n",
      "Epoch 14/100\n",
      "448/448 [==============================] - 6s - loss: 11.3330 - acc: 0.2969 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 15/100\n",
      "448/448 [==============================] - 6s - loss: 11.1155 - acc: 0.3103 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 16/100\n",
      "448/448 [==============================] - 6s - loss: 11.8008 - acc: 0.2679 - val_loss: 14.3552 - val_acc: 0.1094\n",
      "Epoch 17/100\n",
      "448/448 [==============================] - 6s - loss: 12.3044 - acc: 0.2366 - val_loss: 14.6070 - val_acc: 0.0938\n",
      "Epoch 18/100\n",
      "448/448 [==============================] - 6s - loss: 12.5923 - acc: 0.2188 - val_loss: 14.6070 - val_acc: 0.0938\n",
      "Epoch 19/100\n",
      "448/448 [==============================] - 6s - loss: 12.7362 - acc: 0.2098 - val_loss: 14.6070 - val_acc: 0.0938\n",
      "Epoch 20/100\n",
      "448/448 [==============================] - 6s - loss: 12.7087 - acc: 0.2098 - val_loss: 14.2293 - val_acc: 0.1172\n",
      "Epoch 21/100\n",
      "448/448 [==============================] - 6s - loss: 11.8367 - acc: 0.2656 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 22/100\n",
      "448/448 [==============================] - 6s - loss: 11.6209 - acc: 0.2790 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 23/100\n",
      "448/448 [==============================] - 6s - loss: 11.4410 - acc: 0.2902 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 24/100\n",
      "448/448 [==============================] - 6s - loss: 11.2251 - acc: 0.3036 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 25/100\n",
      "448/448 [==============================] - 6s - loss: 11.3352 - acc: 0.2946 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 26/100\n",
      "448/448 [==============================] - 6s - loss: 11.1531 - acc: 0.3080 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 27/100\n",
      "448/448 [==============================] - 6s - loss: 11.5921 - acc: 0.2790 - val_loss: 11.2071 - val_acc: 0.3047\n",
      "Epoch 28/100\n",
      "448/448 [==============================] - 6s - loss: 11.3690 - acc: 0.2946 - val_loss: 11.7108 - val_acc: 0.2734\n",
      "Epoch 29/100\n",
      "448/448 [==============================] - 6s - loss: 11.2611 - acc: 0.3013 - val_loss: 11.5849 - val_acc: 0.2812\n",
      "Epoch 30/100\n",
      "448/448 [==============================] - 6s - loss: 11.5490 - acc: 0.2835 - val_loss: 11.4590 - val_acc: 0.2891\n",
      "Epoch 31/100\n",
      "448/448 [==============================] - 6s - loss: 11.4086 - acc: 0.2902 - val_loss: 12.9700 - val_acc: 0.1953\n",
      "Epoch 32/100\n",
      "448/448 [==============================] - 6s - loss: 11.4410 - acc: 0.2902 - val_loss: 10.4516 - val_acc: 0.3516\n",
      "Epoch 33/100\n",
      "448/448 [==============================] - 6s - loss: 11.2619 - acc: 0.3013 - val_loss: 10.5775 - val_acc: 0.3438\n",
      "Epoch 34/100\n",
      "448/448 [==============================] - 6s - loss: 11.0913 - acc: 0.3103 - val_loss: 10.5775 - val_acc: 0.3438\n",
      "Epoch 35/100\n",
      "448/448 [==============================] - 6s - loss: 10.9013 - acc: 0.3237 - val_loss: 10.4516 - val_acc: 0.3516\n",
      "Epoch 36/100\n",
      "448/448 [==============================] - 6s - loss: 11.1531 - acc: 0.3080 - val_loss: 10.1997 - val_acc: 0.3672\n",
      "Epoch 37/100\n",
      "448/448 [==============================] - 6s - loss: 11.0812 - acc: 0.3125 - val_loss: 10.1997 - val_acc: 0.3672\n",
      "Epoch 38/100\n",
      "448/448 [==============================] - 6s - loss: 11.1531 - acc: 0.3080 - val_loss: 10.1997 - val_acc: 0.3672\n",
      "Epoch 39/100\n",
      "448/448 [==============================] - 6s - loss: 11.0812 - acc: 0.3125 - val_loss: 10.1997 - val_acc: 0.3672\n",
      "Epoch 40/100\n",
      "448/448 [==============================] - 6s - loss: 10.9503 - acc: 0.3192 - val_loss: 10.5775 - val_acc: 0.3438\n",
      "Epoch 41/100\n",
      "448/448 [==============================] - 6s - loss: 11.2972 - acc: 0.2991 - val_loss: 10.7034 - val_acc: 0.3359\n",
      "Epoch 42/100\n",
      "448/448 [==============================] - 6s - loss: 11.4050 - acc: 0.2924 - val_loss: 10.8293 - val_acc: 0.3281\n",
      "Epoch 43/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.8491 - val_acc: 0.3203\n",
      "Epoch 44/100\n",
      "448/448 [==============================] - 6s - loss: 11.4410 - acc: 0.2902 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 45/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 46/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 47/100\n",
      "448/448 [==============================] - 6s - loss: 11.4050 - acc: 0.2924 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 48/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 49/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 50/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 51/100\n",
      "448/448 [==============================] - 6s - loss: 11.4050 - acc: 0.2924 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 52/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 53/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 54/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 55/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 56/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 57/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 58/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 59/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 60/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 61/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 62/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 63/100\n",
      "448/448 [==============================] - 6s - loss: 11.5489 - acc: 0.2835 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 64/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 66/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 67/100\n",
      "448/448 [==============================] - 6s - loss: 11.4410 - acc: 0.2902 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 68/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 69/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 70/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 71/100\n",
      "448/448 [==============================] - 6s - loss: 11.5489 - acc: 0.2835 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 72/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 73/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 74/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 75/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 76/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 77/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 78/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 79/100\n",
      "448/448 [==============================] - 6s - loss: 11.5489 - acc: 0.2835 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 80/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 81/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 82/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 83/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 84/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 85/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 86/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 87/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 88/100\n",
      "448/448 [==============================] - 6s - loss: 11.5489 - acc: 0.2835 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 89/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 90/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 91/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 92/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 93/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 94/100\n",
      "448/448 [==============================] - 6s - loss: 11.5129 - acc: 0.2857 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 95/100\n",
      "448/448 [==============================] - 6s - loss: 11.5489 - acc: 0.2835 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 96/100\n",
      "448/448 [==============================] - 6s - loss: 11.4769 - acc: 0.2879 - val_loss: 10.9553 - val_acc: 0.3203\n",
      "Epoch 97/100\n",
      "320/448 [====================>.........] - ETA: 1s - loss: 11.5849 - acc: 0.2812"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
